{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 데이터 로드,  초기 경로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "# 현재 폴더의 상위 상위 폴더 경로를 가져옵니다.\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "grandparents_dir = os.path.dirname(parent_dir)\n",
    "\n",
    "# 상위 상위 폴더 경로를 sys.path에 추가합니다.\n",
    "sys.path.insert(0, parent_dir)\n",
    "sys.path.insert(0, grandparents_dir)\n",
    "\n",
    "# 패키지 경로를 직접 import합니다.\n",
    "from socceraction.data.statsbomb import StatsBombLoader\n",
    "import socceraction.spadl as spadl\n",
    "import socceraction.vaep.formula as vaepformula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 15:29:40.203695: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-21 15:29:40.224701: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-21 15:29:40.232936: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-21 15:29:40.256367: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-21 15:29:41.563854: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from nn.td_two_tower_lstm import TD_Prediction_TT_Embed\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils import handle_trace_length, get_together_training_batch, compromise_state_trace_length\n",
    "from configuration import MODEL_TYPE, MAX_TRACE_LENGTH, FEATURE_NUMBER, BATCH_SIZE, GAMMA, H_SIZE, \\\n",
    "    model_train_continue, FEATURE_TYPE, ITERATE_NUM, learning_rate, SPORT, save_mother_dir, hidden_dim, lr, gamma, batch_size, memory_size, max_trace_length, output_dim, num_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gim_folder = os.path.dirname(os.getcwd())\n",
    "\n",
    "LOG_DIR = str(gim_folder) + save_mother_dir + \"/models/hybrid_sl_log_NN/Scale-three-cut_together_log_train_feature\" + str(\n",
    "    FEATURE_TYPE) + \"_batch\" + str(\n",
    "    BATCH_SIZE) + \"_iterate\" + str(\n",
    "    ITERATE_NUM) + \"_lr\" + str(\n",
    "    learning_rate) + \"_\" + str(MODEL_TYPE) + \"_MaxTL\" + str(MAX_TRACE_LENGTH)\n",
    "SAVED_NETWORK = str(gim_folder) + save_mother_dir + \"/models/hybrid_sl_saved_NN/Scale-three-cut_together_saved_networks_feature\" + str(\n",
    "    FEATURE_TYPE) + \"_batch\" + str(\n",
    "    BATCH_SIZE) + \"_iterate\" + str(\n",
    "    ITERATE_NUM) + \"_lr\" + str(\n",
    "    learning_rate) + \"_\" + str(MODEL_TYPE) + \"_MaxTL\" + str(MAX_TRACE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'총 게임 수 : 64'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATA_STORE = \"../datastore\"\n",
    "DIR_GAMES_ALL = os.listdir(DATA_STORE)\n",
    "number_of_total_game = len(DIR_GAMES_ALL)\n",
    "\n",
    "display(f\"총 게임 수 : {number_of_total_game}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train_network와 Cost/Plot 함수 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost 저장\n",
    "def write_game_average_csv(data_record):\n",
    "    \"\"\"\n",
    "    write the cost of training\n",
    "    :param data_record: the recorded cost dict\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if os.path.exists(LOG_DIR + '/avg_cost_record.csv'):\n",
    "            with open(LOG_DIR + '/avg_cost_record.csv', 'a') as csvfile:\n",
    "                fieldnames = (data_record[0]).keys()\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                for record in data_record:\n",
    "                    writer.writerow(record)\n",
    "        else:\n",
    "            with open(LOG_DIR + '/avg_cost_record.csv', 'w') as csvfile:\n",
    "                fieldnames = (data_record[0]).keys()\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                writer.writeheader()\n",
    "                for record in data_record:\n",
    "                    writer.writerow(record)\n",
    "    except:\n",
    "        if os.path.exists(LOG_DIR + '/avg_cost_record2.csv'):\n",
    "            with open(LOG_DIR + '/avg_cost_record.csv', 'a') as csvfile:\n",
    "                fieldnames = (data_record[0]).keys()\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                for record in data_record:\n",
    "                    writer.writerow(record)\n",
    "        else:\n",
    "            with open(LOG_DIR + '/avg_cost_record2.csv', 'w') as csvfile:\n",
    "                fieldnames = (data_record[0]).keys()\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                writer.writeheader()\n",
    "                for record in data_record:\n",
    "                    writer.writerow(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_qvalue_goal(GTR, GD, SHOT_TRY, state, y, game_id, ITER):\n",
    "    print(f\"len gtr : {len(GTR)}\")\n",
    "    print(f\"len GD : {len(GD)}\")\n",
    "    print(f\"len y : {len(y)}\")\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(15, 10))\n",
    "    ax1.set_xlabel('GTR')\n",
    "    ax1.set_ylabel('Prob')\n",
    "    line1 = ax1.plot(GTR, y[:,0], 'red', label='Home', linestyle=\"--\")\n",
    "    line2 = ax1.plot(GTR, y[:,1], 'blue', label='Away', linestyle=\"--\")\n",
    "    line3 = ax1.plot(GTR, y[:,2], 'yellow', label='Neither', linestyle=\"--\")\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('GD')\n",
    "    line4 = ax2.plot(GTR, GD, 'deeppink', label='Goal Difference')\n",
    "\n",
    "    lines = line1 + line2 + line3 + line4\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax1.legend(lines, labels, loc='upper right')\n",
    "\n",
    "    plt.gca().invert_xaxis()  # x축 반전\n",
    "\n",
    "    # 샷이 일어난 순간을 수직선으로 표시\n",
    "    for shot in SHOT_TRY:\n",
    "        color = 'red' if shot[1] == 1 else 'blue'\n",
    "        shot_team = 'H' if shot[1] == 1 else 'A'\n",
    "        plt.vlines(x=shot[0], ymin=-0.001, ymax=0.003, color=color, linewidth=2, alpha=0.7)\n",
    "        plt.text(shot[0], 0.0035, shot_team, color=color, fontsize=12, ha='center')\n",
    "\n",
    "    plt.title(f'{game_id} Goal Probability')\n",
    "    plt.savefig(f'./plot/goal_prob/{game_id}_{ITER}.jpg')\n",
    "\n",
    "\n",
    "\n",
    "def game_plot(FEATURE_NUMBER, hidden_dim, MAX_TRACE_LENGTH, learning_rate, SAVED_NETWORK, SPORT, MODEL_VERSION, GAME_ID):\n",
    "    # loading network\n",
    "    # 모델 불러오기\n",
    "    model = TD_Prediction_TT_Embed(FEATURE_NUMBER, hidden_dim, MAX_TRACE_LENGTH, learning_rate)\n",
    "    check_path = os.path.join(SAVED_NETWORK, f\"{SPORT}-game-{MODEL_VERSION}.pt\")\n",
    "    checkpoint = torch.load(check_path)  # Load checkpoint\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.optimizer_home.load_state_dict(checkpoint['home_optimizer_state_dict'])\n",
    "    model.optimizer_away.load_state_dict(checkpoint['away_optimizer_state_dict'])\n",
    "    check_point_game_number = checkpoint['check_point_game_number']\n",
    "\n",
    "    # 데이터 로드\n",
    "    game_id = GAME_ID\n",
    "\n",
    "    DATA_STORE = \"../datastore\"\n",
    "    game_list = []\n",
    "    trace_length_list = []\n",
    "    reward_list = []\n",
    "\n",
    "    # load data\n",
    "    dir_game = f'{game_id}'\n",
    "    state = sio.loadmat(DATA_STORE + \"/\" + dir_game + \"/\" + 'rnn_input')\n",
    "    reward = sio.loadmat(DATA_STORE + \"/\" + dir_game + \"/\" + 'rnn_reward')\n",
    "    trace_length = sio.loadmat(DATA_STORE + \"/\" + dir_game + \"/\" + 'trace')\n",
    "    game_list += (state['data'])[0].tolist()\n",
    "    reward_list += (reward['data'])[0].tolist()\n",
    "    trace_length_list += (trace_length['data'])[0].tolist()\n",
    "    \n",
    "    \n",
    "    GTR = []\n",
    "    GD = []\n",
    "    SHOT_TRY = []\n",
    "    for state in game_list:\n",
    "        GTR.append(state[-1][0])\n",
    "        GD.append(state[-1][4])\n",
    "        if state[-1][22] == 1 or state[-1][29] == 1 or state[-1][32] == 1:\n",
    "            SHOT_TRY.append([state[-1][0], round(state[-1][10])])\n",
    "        if state[-1][31] == 1 :\n",
    "            SHOT_TRY.append([state[-1][0], -round(state[-1][10])])        \n",
    "\n",
    "\n",
    "    home_away_indicator = [1 if state[-1][10] > 1 else 0 for state in game_list]\n",
    "    # Prediction\n",
    "    state_trace_length, state_input, reward = compromise_state_trace_length(trace_length_list, game_list, reward_list, MAX_TRACE_LENGTH)\n",
    "\n",
    "\n",
    "    # get the batch variables\n",
    "    y_batch = []\n",
    "\n",
    "    # Target 값 계산\n",
    "    trace_batch_tensor = torch.tensor(state_trace_length, dtype=torch.int32)\n",
    "    s_batch_tensor = torch.tensor(state_input, dtype=torch.float32)\n",
    "    home_away_indicator_tensor = torch.tensor(home_away_indicator, dtype=torch.bool)\n",
    "    # 모델을 평가 모드로 전환\n",
    "    model.eval()\n",
    "    # forward를 통해 출력 계산\n",
    "    with torch.no_grad():\n",
    "        outputs_t0 = model.forward(s_batch_tensor, trace_batch_tensor, home_away_indicator_tensor)\n",
    "    # 필요 시 numpy 배열로 변환 (TensorFlow의 sess.run()과 동일한 역할)\n",
    "    readout_t1_batch = outputs_t0.numpy()\n",
    "    plot_qvalue_goal(GTR, GD, SHOT_TRY, np.array(game_list), readout_t1_batch, game_id, check_point_game_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(model):\n",
    "    \"\"\"\n",
    "    training thr neural network game by game\n",
    "    :param sess: session of tf\n",
    "    :param model: nn model\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    game_number = 0\n",
    "    global_counter = 0\n",
    "    converge_flag = False\n",
    "\n",
    "    # Check if CUDA is available and use GPU if possible\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # model.to(device)\n",
    "\n",
    "    # instance of Pytorch Model\n",
    "    writer = SummaryWriter(LOG_DIR)\n",
    "\n",
    "    # loading network\n",
    "    if model_train_continue:\n",
    "        check_path = os.path.join(SAVED_NETWORK, f\"{SPORT}-game-{1900}.pt\")\n",
    "        checkpoint = torch.load(check_path)  # Load checkpoint\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.optimizer_home.load_state_dict(checkpoint['home_optimizer_state_dict'])\n",
    "        model.optimizer_away.load_state_dict(checkpoint['away_optimizer_state_dict'])\n",
    "        check_point_game_number = checkpoint['check_point_game_number']\n",
    "        epoch = checkpoint['epoch']\n",
    "        game_number_checkpoint = check_point_game_number % number_of_total_game\n",
    "        game_number = check_point_game_number\n",
    "        game_starting_point = 0\n",
    "        print(f\"Interation Now : {epoch}\")\n",
    "        print(\"Successfully loaded:\", SAVED_NETWORK)\n",
    "    else:\n",
    "        print(\"Could not find old network weights\")\n",
    "\n",
    "    game_diff_record_all = []\n",
    "\n",
    "    # 모든 게임 - 30 iteration\n",
    "    while True:\n",
    "        game_diff_record_dict = {}\n",
    "        iteration_now = game_number / number_of_total_game + 1\n",
    "        game_diff_record_dict.update({\"Iteration\": iteration_now})\n",
    "        if converge_flag:\n",
    "            break\n",
    "        elif game_number >= number_of_total_game * ITERATE_NUM:\n",
    "            break\n",
    "        else:\n",
    "            converge_flag = True\n",
    "        # 게임 불러오기\n",
    "        for dir_game in DIR_GAMES_ALL:\n",
    "\n",
    "            if model_train_continue:\n",
    "                if checkpoint and 135:  # go the check point data\n",
    "                    game_starting_point += 1\n",
    "                    if game_number_checkpoint + 1 > game_starting_point:\n",
    "                        continue\n",
    "\n",
    "            v_diff_record = []\n",
    "            game_number += 1\n",
    "            game_cost_record = []\n",
    "            game_files = os.listdir(DATA_STORE + \"/\" + dir_game)\n",
    "            # 게임 안의 episode 별로 reward, input, trace가 저장되어 있다.\n",
    "            for filename in game_files:\n",
    "                if \"rnn_reward\" in filename:\n",
    "                    reward_name = filename\n",
    "                elif \"rnn_input\" in filename:\n",
    "                    state_input_name = filename\n",
    "                elif \"trace\" in filename:\n",
    "                    state_trace_length_name = filename\n",
    "\n",
    "            reward = sio.loadmat(DATA_STORE + \"/\" + dir_game + \"/\" + reward_name)\n",
    "            reward = (reward['data'])[0]\n",
    "\n",
    "            try:\n",
    "                print(f\"{dir_game} episode length : {len(reward)}\")\n",
    "            except:\n",
    "                print(\"\\n\" + dir_game)\n",
    "                raise ValueError(\"reward wrong\")\n",
    "            \n",
    "            # state_input 로드\n",
    "            state_input = sio.loadmat(DATA_STORE + \"/\" + dir_game + \"/\" + state_input_name)\n",
    "            state_input = (state_input['data'])[0]\n",
    "            state_trace_length = sio.loadmat(DATA_STORE + \"/\" + dir_game + \"/\" + state_trace_length_name)\n",
    "            state_trace_length = (state_trace_length['data'])[0]\n",
    "            state_trace_length, state_input, reward = compromise_state_trace_length(state_trace_length, state_input,\n",
    "                                                                                    reward, MAX_TRACE_LENGTH)\n",
    "\n",
    "            reward_count = len(reward)\n",
    "            print(\"reward number\" + str(reward_count))\n",
    "            print(\"=> load file\" + str(dir_game) + \" success\")\n",
    "            if len(state_input) != len(reward) or len(state_trace_length) != len(reward):\n",
    "                raise Exception('state length does not equal to reward length')\n",
    "\n",
    "            train_len = len(state_input)\n",
    "            train_number = 0\n",
    "            s_t0 = state_input[train_number]\n",
    "            train_number += 1\n",
    "\n",
    "            while True:\n",
    "                # try:\n",
    "                batch_return, train_number, s_tl, home_away_indicator = get_together_training_batch(s_t0,\n",
    "                                                                                    state_input,\n",
    "                                                                                    reward,\n",
    "                                                                                    train_number,\n",
    "                                                                                    train_len,\n",
    "                                                                                    state_trace_length,\n",
    "                                                                                    BATCH_SIZE)\n",
    "\n",
    "                # Convert lists to numpy arrays first\n",
    "                s_t0_batch = np.array([d[0] for d in batch_return], dtype=np.float32)\n",
    "                s_t1_batch = np.array([d[1] for d in batch_return], dtype=np.float32)\n",
    "                r_t_batch = np.array([d[2] for d in batch_return], dtype=np.float32)\n",
    "                trace_t0_batch = np.array([d[3] for d in batch_return], dtype=np.int32)\n",
    "                trace_t1_batch = np.array([d[4] for d in batch_return], dtype=np.int32)\n",
    "                y_batch = []\n",
    "\n",
    "                # Target 값 계산\n",
    "                trace_t1_batch_tensor = torch.tensor(trace_t1_batch, dtype=torch.int32)\n",
    "                s_t1_batch_tensor = torch.tensor(s_t1_batch, dtype=torch.float32)\n",
    "                home_away_indicator_tensor = torch.tensor(home_away_indicator, dtype=torch.bool)\n",
    "                # forward pass를 통해 출력 계산\n",
    "                with torch.no_grad():\n",
    "                    outputs_t1 = model.forward(s_t1_batch_tensor, trace_t1_batch_tensor, home_away_indicator_tensor[:, 1])\n",
    "                # 홈/어웨이 출력 선택에 따라 readout_t1_batch 결정\n",
    "                # readout_t1_batch = torch.where(home_away_indicator_tensor[:, 1], outputs_t1[:, 0], outputs_t1[:, 1])\n",
    "\n",
    "                # 필요 시 numpy 배열로 변환 (TensorFlow의 sess.run()과 동일한 역할)\n",
    "                outputs_t1 = outputs_t1.detach()\n",
    "                readout_t1_batch = outputs_t1.numpy()\n",
    "\n",
    "                # print(f\"readout_t1_batch : {readout_t1_batch}\")\n",
    "\n",
    "                for i in range(0, len(batch_return)):\n",
    "                    terminal = batch_return[i][5]\n",
    "                    cut = batch_return[i][6]\n",
    "                    # if terminal, only equals reward\n",
    "                    if terminal or cut:\n",
    "                        y_home = float((r_t_batch[i])[0])\n",
    "                        y_away = float((r_t_batch[i])[1])\n",
    "                        y_end = float((r_t_batch[i])[2])\n",
    "                        y_batch.append([y_home, y_away, y_end])\n",
    "                        break\n",
    "                    else:\n",
    "                        y_home = float((r_t_batch[i])[0]) + GAMMA * (readout_t1_batch[i]).tolist()[0]\n",
    "                        y_away = float((r_t_batch[i])[1]) + GAMMA * (readout_t1_batch[i]).tolist()[1]\n",
    "                        y_end = float((r_t_batch[i])[2]) + GAMMA * (readout_t1_batch[i]).tolist()[2]\n",
    "\n",
    "                        wandb.log({\"Home_prob\": (readout_t1_batch[i]).tolist()[0]})\n",
    "                        wandb.log({\"Away_prob\": (readout_t1_batch[i]).tolist()[1]})\n",
    "                        wandb.log({\"End_prob\": (readout_t1_batch[i]).tolist()[2]})\n",
    "                        # print(f\"no terminal or cut : {[y_home, y_away, y_end]}\")\n",
    "                        y_batch.append([y_home, y_away, y_end])\n",
    "\n",
    "                # y_batch를 텐서로 변환\n",
    "                y_batch_tensor = torch.tensor(y_batch, dtype=torch.float32)\n",
    "                # trace_t0_batch와 s_t0_batch를 텐서로 변환\n",
    "                trace_t0_batch_tensor = torch.tensor(trace_t0_batch, dtype=torch.int32)\n",
    "                s_t0_batch_tensor = torch.tensor(s_t0_batch, dtype=torch.float32)\n",
    "\n",
    "                # s_t0 - forward | 손실 계산\n",
    "                home_loss, away_loss, read_out = model.train_step(s_t0_batch_tensor, trace_t0_batch_tensor, home_away_indicator_tensor[:, 0], y_batch_tensor)\n",
    "                \n",
    "                # 출력 및 디버깅 정보\n",
    "                diff = torch.mean(torch.abs(y_batch_tensor - read_out)).item()\n",
    "                cost_out = torch.mean(torch.square(y_batch_tensor - read_out)).item()\n",
    "                wandb.log({\"td_diff\": diff})\n",
    "                wandb.log({\"td_cost\": cost_out})\n",
    "\n",
    "                v_diff_record.append(diff)\n",
    "\n",
    "                if cost_out > 0.0001:\n",
    "                    converge_flag = False\n",
    "                global_counter += 1\n",
    "                game_cost_record.append(cost_out)\n",
    "                # global_step 및 summary_train에 해당하는 값을 TensorBoard에 기록\n",
    "                writer.add_scalar('Loss/train', cost_out, global_step=global_counter)\n",
    "                writer.add_scalar('Diff/train', diff, global_step=global_counter)\n",
    "                s_t0 = s_tl\n",
    "\n",
    "                # print info\n",
    "                if terminal or ((train_number - 1) / BATCH_SIZE) % 5 == 1:\n",
    "                    print(\"TIMESTEP:\", train_number, \"Game:\", game_number)\n",
    "                    home_avg = sum(read_out[:, 0]) / len(read_out[:, 0])\n",
    "                    away_avg = sum(read_out[:, 1]) / len(read_out[:, 1])\n",
    "                    end_avg = sum(read_out[:, 2]) / len(read_out[:, 2])\n",
    "                    print(\"home average:{0}, away average:{1}, end average:{2}\".format(str(home_avg), str(away_avg),\n",
    "                                                                                       str(end_avg)))\n",
    "                    print(\"cost of the network is\" + str(cost_out))\n",
    "\n",
    "                if terminal:\n",
    "                    v_diff_record_average = sum(v_diff_record) / len(v_diff_record)\n",
    "                    game_diff_record_dict.update({dir_game: v_diff_record_average})\n",
    "\n",
    "\n",
    "                    if game_number % 100 == 0:\n",
    "                        # save progress after a game\n",
    "                        save_path = os.path.join(SAVED_NETWORK, f\"{SPORT}-game-{game_number}.pt\")\n",
    "                        # 모델의 상태 딕셔너리(가중치 등)를 저장\n",
    "                        torch.save({\n",
    "                            'model_state_dict': model.state_dict(),\n",
    "                            'home_optimizer_state_dict' : model.optimizer_home.state_dict(),\n",
    "                            'away_optimizer_state_dict' : model.optimizer_away.state_dict(),\n",
    "                            'check_point_game_number': game_number,\n",
    "                            'global_step': global_counter,\n",
    "                            'epoch' : iteration_now\n",
    "                        }, save_path)\n",
    "                        # plot 그리기\n",
    "                        for game_id in [7537, 7550, 8649]:\n",
    "                            game_plot(FEATURE_NUMBER, hidden_dim, MAX_TRACE_LENGTH, learning_rate, SAVED_NETWORK, SPORT, game_number, game_id)\n",
    "                    break\n",
    "\n",
    "            cost_per_game_average = sum(game_cost_record) / len(game_cost_record)\n",
    "            write_game_average_csv([{\"iteration\": str(game_number / number_of_total_game + 1), \"game\": game_number,\n",
    "                                     \"cost_per_game_average\": cost_per_game_average}])\n",
    "\n",
    "        game_diff_record_all.append(game_diff_record_dict)\n",
    "\n",
    "    # 마지막 모델\n",
    "    # save progress after a game\n",
    "    save_path = os.path.join(SAVED_NETWORK, f\"{SPORT}-game-{game_number}.pt\")\n",
    "    # 모델의 상태 딕셔너리(가중치 등)를 저장\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'home_optimizer_state_dict' : model.optimizer_home.state_dict(),\n",
    "        'away_optimizer_state_dict' : model.optimizer_away.state_dict(),\n",
    "        'check_point_game_number': game_number,\n",
    "        'global_step': global_counter,\n",
    "        'epoch' : iteration_now\n",
    "    }, save_path)\n",
    "    # plot 그리기\n",
    "    for game_id in [7537, 7550, 8649]:\n",
    "        game_plot(FEATURE_NUMBER, hidden_dim, MAX_TRACE_LENGTH, learning_rate, SAVED_NETWORK, SPORT, game_number, game_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_start():\n",
    "\n",
    "    if not os.path.exists(LOG_DIR):\n",
    "        os.makedirs(LOG_DIR)\n",
    "    if not os.path.exists(SAVED_NETWORK):\n",
    "        os.makedirs(SAVED_NETWORK)\n",
    "\n",
    "\n",
    "    if MODEL_TYPE == \"two_tower\":\n",
    "        model = TD_Prediction_TT_Embed(FEATURE_NUMBER, hidden_dim, MAX_TRACE_LENGTH, learning_rate)\n",
    "    else:\n",
    "        raise ValueError(\"MODEL_TYPE error\")\n",
    "    train_network(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 학습 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    train_start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CIDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
