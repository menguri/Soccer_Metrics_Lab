{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x7f3b2a246810>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import random as ran\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from configuration import state_dim, action_dim, hidden_dim, lr, gamma, epsilon, batch_size, memory_size, max_trace_length, output_dim, num_layers, embedding_dim, em_size\n",
    "from nn.two_tower_lstm import TwoTowerLSTM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# from skimage.transform import resize\n",
    "# from skimage.color import rgb2gray\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_STORE = \"./datastore\"\n",
    "\n",
    "DIR_GAMES_ALL = os.listdir(DATA_STORE)\n",
    "number_of_total_game = len(DIR_GAMES_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3,4]\n",
    "a[2:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[]]\n",
      "Epoch:['1', '2', '3', '4', '5', '6', '7']\n"
     ]
    }
   ],
   "source": [
    "from model import SarsaLSTMAgent\n",
    "\n",
    "# SARSA 기반 Main() 함수 구현을 통해 학습 실행\n",
    "def main():\n",
    "    agent = SarsaLSTMAgent(state_dim, action_dim, hidden_dim, lr, gamma, epsilon, batch_size, memory_size, max_trace_length, output_dim, num_layers, embedding_dim, em_size)\n",
    "    epoch = 0\n",
    "    # Train agent during 100 epoch\n",
    "    while epoch < 1:\n",
    "        epoch += 1\n",
    "        # episode = num(goal sequence) | \"We divide a soccer game into goal-scoring episodes\"\n",
    "        for game in [7525]:\n",
    "            episode = os.listdir(DATA_STORE + f'/{game}')\n",
    "            for epi in [1]:\n",
    "                # load data\n",
    "                s = np.load(DATA_STORE + f'/{game}/{epi}/state.npy', allow_pickle=True)\n",
    "                r = np.load(DATA_STORE + f'/{game}/{epi}/reward.npy', allow_pickle=True)\n",
    "                a = np.load(DATA_STORE + f'/{game}/{epi}/action.npy', allow_pickle=True)\n",
    "                # Memory -> T(state_t, action_t, reward_t+1, state_t+1, action_t+1, Team, Trace_Length)\n",
    "                # 이렇게 되면 한 턴만에 빼앗기는 경우, Q를 계산하지 않음.\n",
    "                trace_length = 1\n",
    "                for t in range(len(s)):\n",
    "                    # t이 마지막인 경우, 종료\n",
    "                    if sum(r[t]) == 1:\n",
    "                        break\n",
    "                    # t's team == t+1's team\n",
    "                    if s[t][-1] == s[t+1][-1]:\n",
    "                        agent.store_transition(s[t], a[t], r[t+1], s[t+1], a[t+1], s[t][-1], trace_length)\n",
    "                        trace_length += 1\n",
    "                    else:\n",
    "                        # t의 team과 t+1의 team이 다른 경우, trace_length 초기화\n",
    "                        trace_length = 0\n",
    "\n",
    "                batch = agent.sample_batch()\n",
    "                print(batch)\n",
    "                # print(mi)\n",
    "                # agent update(calc_q_loss)\n",
    "                # agent.update_model()\n",
    "                # # agent's memory init\n",
    "                # agent.store_init()\n",
    "\n",
    "        print(\"Epoch:{}\".format(episode))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sarsa_lstm(agent, num_episodes=1):\n",
    "    for episode in range(num_episodes):\n",
    "        trace = []\n",
    "        \n",
    "        # 하나의 에피소드에서 minibatch를 수행\n",
    "        # 조건 1. t/t+1은 모두 같은 팀이 수행한 행동일 것 \n",
    "        # 조건 2. trace length를 고려해서 batch를 잘라야 함\n",
    "        # 조건 3. 하나의 state(t)는 sequence를 담고 있음. \n",
    "        for event in episode_data:\n",
    "            state, action, reward, next_state, done = event\n",
    "            next_action = agent.select_action(next_state)\n",
    "            \n",
    "            trace.append((state, action, reward, next_state, next_action, done))\n",
    "            if len(trace) == agent.trace_length or done:\n",
    "                for t in trace:\n",
    "                    agent.store_transition(*t)\n",
    "                agent.update_model()\n",
    "                trace = []\n",
    "                \n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        print(f\"Episode {episode + 1}, Total Reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded output shape: torch.Size([4, 300])\n",
      "Embedded output: tensor([[-1.4593, -0.2077,  0.7009,  ...,  0.4990, -0.6829, -0.1326],\n",
      "        [-0.0071,  0.4374,  0.1842,  ..., -0.1883,  0.7059, -2.1017],\n",
      "        [-0.5948,  0.4116, -1.9910,  ..., -0.0759,  2.3095, -0.7185],\n",
      "        [-1.1674, -1.1267,  0.9069,  ...,  0.0464,  2.4247, -1.1143]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 단어 사전 크기와 임베딩 차원 정의\n",
    "vocab_size = 10000  # 단어 사전 크기\n",
    "embedding_dim = 300  # 임베딩 차원\n",
    "\n",
    "# 임베딩 레이어 생성\n",
    "embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "# 예제 입력 (단어 인덱스)\n",
    "input_indices = torch.tensor([1, 2, 3, 4])  # 예: 단어 인덱스 시퀀스\n",
    "\n",
    "# 임베딩 레이어를 통한 입력 변환\n",
    "embedded_output = embedding_layer(input_indices)\n",
    "\n",
    "print(f\"Embedded output shape: {embedded_output.shape}\")  # (시퀀스 길이, 임베딩 차원)\n",
    "print(f\"Embedded output: {embedded_output}\")  # 임베딩된 벡터 출력"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CIDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
