
[34m[1mwandb[39m[22m: [33mWARNING[39m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.
team : 2
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.3209, 0.3522, 0.3269]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[0.3223, 0.3523, 0.3255]], grad_fn=<SoftmaxBackward0>)
team : 1
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.3231, 0.3456, 0.3312]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[0.3234, 0.3445, 0.3321]], grad_fn=<SoftmaxBackward0>)
team : 1
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.3333, 0.3333, 0.3333]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[0.3333, 0.3333, 0.3333]], grad_fn=<SoftmaxBackward0>)
team : 1
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.3333, 0.3333, 0.3333]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[0.3333, 0.3333, 0.3333]], grad_fn=<SoftmaxBackward0>)
team : 2
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.3108, 0.3783, 0.3108]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[0.3077, 0.3853, 0.3070]], grad_fn=<SoftmaxBackward0>)
team : 2
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.3103, 0.3793, 0.3103]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[0.3008, 0.4015, 0.2976]], grad_fn=<SoftmaxBackward0>)
team : 1
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.3333, 0.3333, 0.3333]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[0.3333, 0.3333, 0.3333]], grad_fn=<SoftmaxBackward0>)
team : 1
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.3333, 0.3333, 0.3333]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[0.3333, 0.3333, 0.3333]], grad_fn=<SoftmaxBackward0>)
team : 1
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.3333, 0.3333, 0.3333]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[0.3333, 0.3333, 0.3333]], grad_fn=<SoftmaxBackward0>)
team : 2
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.2794, 0.4628, 0.2578]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[0.2793, 0.4634, 0.2574]], grad_fn=<SoftmaxBackward0>)
team : 1
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.3365, 0.3318, 0.3318]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[0.3366, 0.3317, 0.3317]], grad_fn=<SoftmaxBackward0>)
team : 2
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.2522, 0.5183, 0.2295]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[0.2522, 0.5183, 0.2295]], grad_fn=<SoftmaxBackward0>)
team : 2
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.2220, 0.5769, 0.2011]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[0.2211, 0.5788, 0.2001]], grad_fn=<SoftmaxBackward0>)
team : 2
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.2056, 0.6021, 0.1923]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[0.1950, 0.6241, 0.1809]], grad_fn=<SoftmaxBackward0>)
team : 2
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.1296, 0.7512, 0.1193]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[0.1296, 0.7512, 0.1193]], grad_fn=<SoftmaxBackward0>)
team : 2
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.2000, 0.6024, 0.1976]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[0.1592, 0.6863, 0.1545]], grad_fn=<SoftmaxBackward0>)
team : 2
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.0584, 0.8834, 0.0582]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[0.0546, 0.8912, 0.0543]], grad_fn=<SoftmaxBackward0>)
team : 2
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.0275, 0.9450, 0.0275]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[0.0275, 0.9450, 0.0275]], grad_fn=<SoftmaxBackward0>)
team : 2
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.0154, 0.9692, 0.0154]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[0.0154, 0.9692, 0.0154]], grad_fn=<SoftmaxBackward0>)
team : 1
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.3546, 0.3227, 0.3227]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[0.3569, 0.3215, 0.3215]], grad_fn=<SoftmaxBackward0>)
team : 2
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.0242, 0.9517, 0.0242]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[0.0159, 0.9681, 0.0159]], grad_fn=<SoftmaxBackward0>)
team : 2
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.0299, 0.9402, 0.0299]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[0.0140, 0.9720, 0.0140]], grad_fn=<SoftmaxBackward0>)
team : 1
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.3879, 0.3060, 0.3060]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[0.3902, 0.3049, 0.3049]], grad_fn=<SoftmaxBackward0>)
team : 1
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.4407, 0.2796, 0.2796]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[0.4407, 0.2796, 0.2796]], grad_fn=<SoftmaxBackward0>)
team : 2
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.0028, 0.9944, 0.0028]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[0.0027, 0.9945, 0.0027]], grad_fn=<SoftmaxBackward0>)
team : 2
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.1674, 0.6652, 0.1674]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[0.0441, 0.9118, 0.0441]], grad_fn=<SoftmaxBackward0>)
team : 2
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[7.6582e-04, 9.9847e-01, 7.6582e-04]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[7.6582e-04, 9.9847e-01, 7.6582e-04]], grad_fn=<SoftmaxBackward0>)
team : 2
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[8.8215e-04, 9.9824e-01, 8.8215e-04]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[5.4568e-04, 9.9891e-01, 5.4568e-04]], grad_fn=<SoftmaxBackward0>)
team : 2
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.1144, 0.7712, 0.1144]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[0.0123, 0.9754, 0.0123]], grad_fn=<SoftmaxBackward0>)
team : 2
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[1.1266e-04, 9.9977e-01, 1.1266e-04]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[1.1068e-04, 9.9978e-01, 1.1068e-04]], grad_fn=<SoftmaxBackward0>)
team : 1
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.4411, 0.2795, 0.2795]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[0.4583, 0.2709, 0.2709]], grad_fn=<SoftmaxBackward0>)
team : 1
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.3895, 0.3053, 0.3053]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[0.4464, 0.2768, 0.2768]], grad_fn=<SoftmaxBackward0>)
team : 1
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.4103, 0.2948, 0.2948]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[0.4864, 0.2568, 0.2568]], grad_fn=<SoftmaxBackward0>)
team : 1
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.5367, 0.2317, 0.2317]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[0.6022, 0.1989, 0.1989]], grad_fn=<SoftmaxBackward0>)
team : 2
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.0775, 0.8450, 0.0775]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[0.0039, 0.9922, 0.0039]], grad_fn=<SoftmaxBackward0>)
team : 1
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.7272, 0.1364, 0.1364]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[0.7548, 0.1226, 0.1226]], grad_fn=<SoftmaxBackward0>)
team : 2
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[4.3404e-05, 9.9991e-01, 4.3404e-05]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[3.1281e-05, 9.9994e-01, 3.1281e-05]], grad_fn=<SoftmaxBackward0>)
team : 2
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[9.5737e-04, 9.9809e-01, 9.5737e-04]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[5.7970e-05, 9.9988e-01, 5.7970e-05]], grad_fn=<SoftmaxBackward0>)
team : 2
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[6.5635e-06, 9.9999e-01, 6.5635e-06]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[6.5435e-06, 9.9999e-01, 6.5435e-06]], grad_fn=<SoftmaxBackward0>)
team : 2
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[3.4365e-06, 9.9999e-01, 3.4365e-06]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[3.4365e-06, 9.9999e-01, 3.4365e-06]], grad_fn=<SoftmaxBackward0>)
team : 2
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.0204, 0.9591, 0.0204]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[1.5172e-04, 9.9970e-01, 1.5172e-04]], grad_fn=<SoftmaxBackward0>)
team : 1
reward : tensor([[[1., 0., 0.]]]) / reward : 1.0
q_values_a : tensor([[0.8604, 0.0698, 0.0698]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[0.8696, 0.0652, 0.0652]], grad_fn=<SoftmaxBackward0>)
7525 game's 1 episode >> Home loss: 0.06454892191616218 | Away loss : 0.005200550654671612
team : 1
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.5351, 0.2325, 0.2325]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[0.7211, 0.1395, 0.1395]], grad_fn=<SoftmaxBackward0>)
team : 2
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[8.7798e-05, 9.9982e-01, 8.7798e-05]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[3.8058e-06, 9.9999e-01, 3.8058e-06]], grad_fn=<SoftmaxBackward0>)
team : 1
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.5694, 0.2153, 0.2153]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[0.7759, 0.1120, 0.1120]], grad_fn=<SoftmaxBackward0>)
team : 1
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.9635, 0.0183, 0.0183]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[0.9692, 0.0154, 0.0154]], grad_fn=<SoftmaxBackward0>)
team : 2
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[6.6786e-07, 1.0000e+00, 6.6786e-07]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[6.6786e-07, 1.0000e+00, 6.6786e-07]], grad_fn=<SoftmaxBackward0>)
team : 2
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[4.0698e-07, 1.0000e+00, 4.0698e-07]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[4.0698e-07, 1.0000e+00, 4.0698e-07]], grad_fn=<SoftmaxBackward0>)
team : 1
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.6458, 0.1771, 0.1771]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[0.8703, 0.0648, 0.0648]], grad_fn=<SoftmaxBackward0>)
team : 1
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.9822, 0.0089, 0.0089]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[0.9873, 0.0063, 0.0063]], grad_fn=<SoftmaxBackward0>)
team : 1
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.9790, 0.0105, 0.0105]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[0.9894, 0.0053, 0.0053]], grad_fn=<SoftmaxBackward0>)
team : 2
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.0081, 0.9838, 0.0081]], grad_fn=<SoftmaxBackward0>)
next_q_values_a : tensor([[2.0865e-05, 9.9996e-01, 2.0865e-05]], grad_fn=<SoftmaxBackward0>)
team : 2
reward : tensor([[[0., 0., 0.]]]) / reward : 0.0
q_values_a : tensor([[0.0065, 0.9869, 0.0065]], grad_fn=<SoftmaxBackward0>)
